{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Semantic Segmentation Algorithm - v4 - misc text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation](#Data-Preparation)\n",
    "  1. [Download data](#Download-data)\n",
    "  2. [Setup data](#Setup-data)\n",
    "  3. [Upload to S3](#Upload-to-S3)\n",
    "4. [Training](#Training)\n",
    "5. [Deployment](#Deployment)\n",
    "6. [Inference](#Inference)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Semantic Segmentation (SS) is the task of classifying every pixel in an image with a class from a known set of labels. In contrast, [image classification](https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining-highlevel.html) generates only one label per image and [object detection](https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/object_detection_birds/object_detection_birds.html) generates a bounding box along with the label for each object in the image. The semantic segmentation output is usually represented as different pixel values in the image. Therefore, the output is an integer matrix (or a grayscale image) with the same shape as the input image. This output image is also called a segmentation mask. With the Amazon SageMaker Semantic Segmentation algorithm, not only can you train your models with your own dataset but also use our pre-trained models for lazy initialization.\n",
    "\n",
    "This notebook is an end-to-end example introducing the Amazon SageMaker Semantic Segmentation algorithm. In this demo, we will demonstrate how to train and host a semantic segmentation model using the fully-convolutional network ([FCN](https://arxiv.org/abs/1605.06211)) algorithm using the [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/) for training. Amazon SageMaker Semantic Segmentation also provides the option of using Pyramid Scene Parsing Network ([PSP](https://arxiv.org/abs/1612.01105)) and [Deeplab-v3](https://arxiv.org/abs/1706.05587) in addition to the FCN Network. Along the way, we will also demonstrate how to construct a training dataset in the format that the training job will consume. Finally, we will demonstrate how to host and validate the trained model.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This notebook has been updated for [SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/) (`sagemaker`) **version 2** - which introduces some breaking changes from the v1 API. See the [migration guide](https://sagemaker.readthedocs.io/en/stable/v2.html) for details.\n",
    "\n",
    "In this first cell we'll upgrade the SageMaker SDK installation if needed, before we `import sagemaker`. You can skip this cell if your environment as already up-to-date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the Semantic Segmentation algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with, we need an AWS account role with SageMaker access. This role that is used to give SageMaker access to your data in S3 can automatically be obtained from the role used to start the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the S3 bucket that is used to store training data and the trained model artifacts. In this notebook, we use the default bucket that comes with Sagemaker. However, you can also create a bucket and use that bucket instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need the location of the Amazon SageMaker Semantic Segmentaion algorithm's docker image, which the SDK can look up for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "[Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) is a popular computer vision dataset which is used for annual semantic segmentation challenges from 2005 to 2012. The dataset has 1464 training and 1449 validation images with 21 classes. Examples of the segmentation dataset can be seen in the [Pascal VOC Dataset page](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html). The classes are as follows:\n",
    "\n",
    "| Label Id |     Class     |\n",
    "|:--------:|:-------------:|\n",
    "|     0    |   Background  |\n",
    "|     1    |   Aeroplane   |\n",
    "|     2    |    Bicycle    |\n",
    "|     3    |      Bird     |\n",
    "|     4    |      Boat     |\n",
    "|    5     |     Bottle    |\n",
    "|     6    |      Bus      |\n",
    "|     7    |      Car      |\n",
    "|     8    |      Cat      |\n",
    "|     9    |     Chair     |\n",
    "|    10    |      Cow      |\n",
    "|    11    |  Dining Table |\n",
    "|    12    |      Dog      |\n",
    "|    13    |     Horse     |\n",
    "|    14    |   Motorbike   |\n",
    "|    15    |     Person    |\n",
    "|    16    |  Potted Plant |\n",
    "|    17    |     Sheep     |\n",
    "|    18    |      Sofa     |\n",
    "|    19    |     Train     |\n",
    "|    20    |  TV / Monitor |\n",
    "|    255   | Hole / Ignore |\n",
    "\n",
    "In this notebook, we will use the data sets from 2012. While using the Pascal VOC dataset, please be aware of the  usage rights:\n",
    "\"The VOC data includes images obtained from the \"flickr\" website. Use of these images must respect the corresponding terms of use: \n",
    "* \"flickr\" terms of use (https://www.flickr.com/help/terms)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Let us download the Pascal VOC 2012 datasets.\n",
    "\n",
    "If this notebook was run before, you may have downloaded some data and set the files up already - in which case you can skip over this section and don't need to run it all over again.\n",
    "\n",
    "If you'd like to **clear** previously downloaded data before re-downloading, *un-comment* the cell below and run it first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the Pascal VOC 2012 dataset homepage (including downloads) at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/\n",
    "\n",
    "Here though, we'll use the [combined VOC2007+2012 archive from FastAI](https://registry.opendata.aws/fast-ai-imagelocal/) listed on the [AWS Open Data Registry](https://aws.amazon.com/opendata/): (and extract only the `VOC2012` folder) ...because even though the overall size is larger, the download is often faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data \n",
    "\n",
    "-> [`sagesemseg/dataset.py`](../../../notebooks_and_scripts/sagesemseg/dataset.py)\n",
    "\n",
    "```bash\n",
    "sagesemseg upload_dataset help\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set up an output location in S3, where the outputs of the training job (including model artifact) will be stored.\n",
    "\n",
    "We'll use another folder in the same S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You can also 'attach' to a previously completed training job if, for example, you had to re-start the\n",
    "# notebook and would like to skip straight to `ss_estimator.deploy()` later on:\n",
    "\n",
    "# ss_estimator = sagemaker.estimator.Estimator.attach(\"ss-notebook-demo-2020-10-29-07-05-36-516\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantic segmentation algorithm at its core has two components.\n",
    "\n",
    "- An encoder or backbone network,\n",
    "- A decoder or algorithm network. \n",
    "\n",
    "The encoder or backbone network is typically a regular convolutional neural network that may or may not have had their layers pre-trained on an alternate task such as the [classification task of ImageNet images](http://www.image-net.org/). The Amazon SageMaker Semantic Segmentation algorithm comes with two choices of pre-trained or to be trained-from-scratch backbone networks ([ResNets](https://arxiv.org/abs/1512.03385) 50 or 101). \n",
    "\n",
    "The decoder is a network that picks up the outputs of one or many layers from the backbone and reconstructs the segmentation mask from it. Amazon SageMaker Semantic Segmentation algorithm comes with a choice of the [Fully-convolutional network (FCN)](https://arxiv.org/abs/1605.06211) or the [Pyramid scene parsing (PSP) network](https://arxiv.org/abs/1612.01105).\n",
    "\n",
    "The algorithm also has ample options for hyperparameters that help configure the training job. The next step in our training, is to setup these networks and hyperparameters along with data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Semantic Segmentation [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html) for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the `epochs`. This defines how many passes of the dataset we iterate over and determines that training time of the algorithm. For the sake of demonstration let us run only `10` epochs. Based on our tests, train the model for `30` epochs with similar settings should give us 'reasonable' segmentation results on the Pascal VOC data. For the most part, we will stick to using the simplest of settings. For more information on the hyperparameters of this algorithm, refer to the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/segmentation-hyperparameters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us define the input data \"channels\" for training - as defined for this algorithm in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html).\n",
    "\n",
    "Each channel can be configured either with a simple `s3://...` string pointing to the data, or by a [sagemaker.inputs.TrainingInput](https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#sagemaker.inputs.TrainingInput) object which provides additional configuration options such as:\n",
    "\n",
    "- How the data is distributed between nodes for multi-instance training jobs\n",
    "- Whether the channel should use [Pipe Mode](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/), and other options\n",
    "\n",
    "The channels are specified in a simple dictionary, specifying what data sources the training job should connect to and how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `Estimator` object with the hyperparameters and data channels for training defined, so the only remaining thing to do is actually train a model.\n",
    "\n",
    "The following simple command will perform several steps:\n",
    "\n",
    "- Firstly, the instances that we requested while creating the `Estimator` above are provisioned and the appropriate container image for the algorithm installed.\n",
    "- Then, the data from our channels are downloaded into the instance.\n",
    "- Once data and container are ready, the training job begins.\n",
    "- Logs from the training will be pulled through to this notebook as it runs, including the loss on the training data (which is the pixel-wise cross-entropy loss as described in the algorithm docs) and the metrics on the validation data (pixel-wise label accuracy and mean intersection-over-union or mIoU)\n",
    "- These logs and metrics will also be published to [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/) - linked from the [Amazon SageMaker Console](https://console.aws.amazon.com/sagemaker/home?#/jobs) for your training job.\n",
    "- When the training is complete, your model artifacts will be uploaded to Amazon S3 in the output location we specified earlier.\n",
    "- As soon as the training job finishes (regardless of whether it succeeds or fails), the infrastructure is terminated - so we only pay for the GPU-accelerated training instance for the time that the job runs. Nice!\n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed below, and your trained model can then be found in the S3 location we set up for the estimator's `output_path`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
